[{"authors":["YifanWang"],"categories":null,"content":"I'm a third-year PhD student in the Interactive Geometry Lab (IGL) at ETH Zurich supervised by Prof. Olga Sorkine-Hornung. My area of research lies in applying machine learning techniques, especially deep learning, to challenging image and geometry processing problems. During my PhD study, I have had the honour to work in the Imaging and Video group at Disney Research and the Creative Intelligence Lab at Adobe Research in Seattle.\nBorn and raised in China, I moved to Europe for study almost 10 years ago. I received my Bachelor degree with distinction in Electrical Engineering and Information Technology (Elektrotechnik und Informationstechnik) at Technische Universität München (TUM) and completed my Master with distinction in Robotics, Systems and Control at ETH Zurich.\n","date":1575936000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1575936000,"objectID":"f91bdc3427753dc0f7ea9e5496fbbd2c","permalink":"https://yifita.github.io/authors/yifanwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yifanwang/","section":"authors","summary":"I'm a third-year PhD student in the Interactive Geometry Lab (IGL) at ETH Zurich supervised by Prof. Olga Sorkine-Hornung. My area of research lies in applying machine learning techniques, especially deep learning, to challenging image and geometry processing problems. During my PhD study, I have had the honour to work in the Imaging and Video group at Disney Research and the Creative Intelligence Lab at Adobe Research in Seattle.\nBorn and raised in China, I moved to Europe for study almost 10 years ago.","tags":null,"title":"Yifan Wang","type":"authors"},{"authors":["Yifan Wang","NoamAigerman","VovaKim","SidChaudhuri","OlgaSorkine"],"categories":null,"content":"We can warp an arbitrary input shape to match the grob structure of an arbitrary target shape, while preserving all the local geometric details. The input and target shape is not required to have dense correspondences, the same topology, or even the same representation form (e.g. points, mesh and 2D image). The key of our method is reducing the degrees of freedom of the deformation space by extending the traditional cage-based deformation technique.\nShape synthesis Deformation transfer ","date":1575936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575936000,"objectID":"56b48b416e5155ffbcd3738365bf7ef5","permalink":"https://yifita.github.io/publication/deep_cage/","publishdate":"2019-12-10T00:00:00Z","relpermalink":"/publication/deep_cage/","section":"publication","summary":"We propose a novel learnable representation for detail-preserving shape deformation extending a traditional cage-based deformation technique. We demonstrate the utility of our method for synthesizing shape variations and deformation transfer.","tags":["deformation","shape modeling","deep learning","3D"],"title":"Neural Cages for Detail-Preserving 3D Deformations","type":"publication"},{"authors":["Victor Cornillère","AbdelazizDjelouah","Yifan Wang","OlgaSorkine","ChristopherSchroers"],"categories":null,"content":"Overview In blind super-resolution, the degradation kernel k applied on the high resolution image to obtain the low resolution image $I_l$ is unknown. Our pipeline is duplicated for two different kernels (a) and (b): the degradation-aware generator ($\\mathcal{F}_g$) computes a high resolution output according to the provided blur kernel k. We note that a NN $\\mathcal{F}_k$ is used to map the kernels to a low dimensional representation. The two kernels will result in different high resolution estimates. The kernel (a) farther from the unknown original degradation leads to more artifacts. To detect this, we propose a kernel discriminator network ($\\mathcal{F}_d$) predicting the error due to using the incorrect kernel. By taking advantage of these two networks, we can express kernel estimation as finding the blur kernel resulting in the least amount of errors and artifacts in the predicted high resolution image (See text for details). Photo Credits: Pixabay/pexels.com.\nResults (screenshots from the paper) ","date":1568073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568073600,"objectID":"5644e904519719091fad771448f41cd1","permalink":"https://yifita.github.io/publication/variational_blindsr/","publishdate":"2019-09-10T00:00:00Z","relpermalink":"/publication/variational_blindsr/","section":"publication","summary":"We propose a novel approach for single image super-resolution to simultaneously predict high resolution images and the degradation kernels.","tags":["image processing","super-resolution","deep learning"],"title":"Blind Image Super-Resolution with Spatially Variant Degradations","type":"publication"},{"authors":["Yifan Wang","Felice Serena","ShihaoWu","CengizOeztireli","OlgaSorkine"],"categories":null,"content":"Shape Synthesis We can synthesize shapes from multiple 2D images. This process is not constrained by topology changes. Point Cloud Filering Using DSS we can directly apply image-based filters to a point cloud to achieve various geometric effect. Point Cloud Denoising We create state-of-the-art point cloud denoising results by marrying our differential renderer with the famous image-to-image translation deep learning framework Pix2Pix. Accompanying Video   Acknowledgement We would like to thank Federico Danieli for the insightful discussion, Philipp Herholz for the timely feedack, Romann Weber for the video voice-over and Derek Liu for the help during the rebuttal. This work was supported in part by gifts from Adobe, Facebook and Snap, Inc.\n","date":1564358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564358400,"objectID":"f9cfe2faadb93867e711425faa964bd3","permalink":"https://yifita.github.io/publication/dss/","publishdate":"2019-07-29T00:00:00Z","relpermalink":"/publication/dss/","section":"publication","summary":"We propose a high-fidelity differentiable renderer for point clouds. We demonstrate how the proposed technique can be used to leverage contemporary deep neural networks to achieve state-of-the-art results in challenging geometry processing tasks.","tags":["geometry processing","point cloud","deep learning","rendering","3D"],"title":"Differentiable Surface Splatting for Point-based Geometry Processing","type":"publication"},{"authors":null,"categories":null,"content":"","date":1561766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561766400,"objectID":"80bb9a74f5dcab69055a451248709b85","permalink":"https://yifita.github.io/project/neural-shape/","publishdate":"2019-06-29T00:00:00Z","relpermalink":"/project/neural-shape/","section":"project","summary":"Representing and generating shapes using neural networks","tags":["Deep Learning","shape modeling","3D"],"title":"Neural Shapes","type":"project"},{"authors":["Yifan Wang","ShihaoWu","HuiHuang","DanielCohenor","OlgaSorkine"],"categories":null,"content":"Results    References PU-Net: L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, \u0026ldquo;Pu-net: Point cloud upsampling network\u0026rdquo;, CVPR 2018\nEC-Net: L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, \u0026ldquo;Ec-net: an edge-aware point set consolidation network\u0026rdquo;, ECCV 2018\nEAR: H. Huang, S. Wu, M. Gong, D. Cohen-Or, U. Ascher, and H. Zhang, \u0026ldquo;Edge-aware point set resampling\u0026rdquo;, ACM ToG 2013\nWLOP: H. Huang, D. Li, H. Zhang, U. Ascher, and D. Cohen-Or, \u0026ldquo;Consolidation of unorganized point clouds for surface reconstruction\u0026rdquo;, SIGGRAPH Asia 2009\n","date":1561334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561334400,"objectID":"ad39d116047628dce95aceeb6af0c9ce","permalink":"https://yifita.github.io/publication/3pu/","publishdate":"2019-06-24T00:00:00Z","relpermalink":"/publication/3pu/","section":"publication","summary":"We present a detail-driven deep neural network for point set upsampling. A high-resolution point set is essential for point-based rendering and surface reconstruction. Inspired by the recent success of neural image super-resolution techniques, we progressively train a cascade of patch-based upsampling networks on different levels of detail end-to-end. We propose a series of architectural design contributions that lead to a substantial performance boost. The effect of each technical contribution is demonstrated in an ablation study. Qualitative and quantitative experiments show that our method significantly outperforms the state-of-the-art learning-based and optimazation-based approaches, both in terms of handling low-resolution inputs and revealing high-fidelity details.","tags":["point cloud","super-resolution","deep learning","geometry processing","3D"],"title":"Patch-base progressive 3D Point Set Upsampling","type":"publication"},{"authors":null,"categories":null,"content":"","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"45275d342533c1f0633acc26122b3285","permalink":"https://yifita.github.io/project/point-geometry/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/project/point-geometry/","section":"project","summary":"Making use of this extremely flexible yet unstructured form of shape represenation.","tags":["Deep Learning","geometry processing","point cloud","3D"],"title":"Point-based geometry processing","type":"project"},{"authors":["Yifan Wang","FedericoPerazzi","BrianMcWilliams","AlexanderHornung","OlgaSorkine","ChristopherSchroers"],"categories":null,"content":"Results  MsLapSRN: Lai, Wei-Sheng, et al. \u0026ldquo;Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks.\u0026rdquo; arXiv preprint arXiv:1710.01992 (2017).\nEDSR, MDSR: Lim, Bee, et al. \u0026ldquo;Enhanced deep residual networks for single image super-resolution.\u0026rdquo; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. Vol. 1. No. 2. 2017.\nRDN:Zhang, Yulun, et al. \u0026ldquo;Residual Dense Network for Image Super-Resolution.\u0026rdquo; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018.\n Featured Video It's a pleasure to be featured in \u0026ldquo;2 minute paper\u0026quot;, an amazing YouTube channel that introduces latest development of AI in a variety of applications. Here's the video that talks about our work.   ","date":1523836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523836800,"objectID":"2375f889a32d2d73dc112f3f464731d5","permalink":"https://yifita.github.io/publication/prosr/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/prosr/","section":"publication","summary":"We set a new benchmark for single-image super-resolution by exploiting progressiveness both in architecture and training. The proposed multi-scale models, **ProSR** and **ProSRGan**, improve the reconstruction quality in terms of PSNR and visual quality respectively. ProSR is one of the winning teams.","tags":["super-resolution","image processing","deep learning","image","2D"],"title":"A Fully Progressive Approach to Single-Image Super-Resolution","type":"publication"},{"authors":null,"categories":null,"content":"","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"105797dda5efbb8aea73eb8475651802","permalink":"https://yifita.github.io/project/single-image-super-resolution/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/project/single-image-super-resolution/","section":"project","summary":"Pushing the limit of \"zoom-in\"s.","tags":["image processing","deep learning","super-resolution","2D"],"title":"Single-Image Super-Resolution","type":"project"},{"authors":["Yifan Wang","JieSong","Limin Wang","Luc Van Gool","OtmarHilliges"],"categories":null,"content":"Network Architecture ","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467331200,"objectID":"766a3c831d4b11b535c5476c69fcdb9f","permalink":"https://yifita.github.io/publication/action_srcnn/","publishdate":"2016-07-01T00:00:00Z","relpermalink":"/publication/action_srcnn/","section":"publication","summary":"We propose a new deep architecture by incorporating object/human detection results into the framework for action recognition, called two-stream semantic region based CNNs (SR-CNNs). We perform experiments on UCF101 dataset and demonstrate its superior performance to the original two-stream CNNs. ","tags":["action recognition","video"],"title":"Two-Stream SR-CNNs for Action Recognition in Videos","type":"publication"}]