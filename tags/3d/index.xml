<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>3D | Yifan Wang - ETH Zurich</title>
    <link>https://yifita.github.io/tags/3d/</link>
      <atom:link href="https://yifita.github.io/tags/3d/index.xml" rel="self" type="application/rss+xml" />
    <description>3D</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 14 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yifita.github.io/img/icon-192.png</url>
      <title>3D</title>
      <link>https://yifita.github.io/tags/3d/</link>
    </image>
    
    <item>
      <title>Neural Cages for Detail-Preserving 3D Deformations</title>
      <link>https://yifita.github.io/publication/deep_cage/</link>
      <pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://yifita.github.io/publication/deep_cage/</guid>
      <description>&lt;h3 id=&#34;we-can-warp-an-arbitrary-input-shape-to-match-the-grob-structure-of-an-arbitrary-target-shape-while-__preserving-all-the-local-geometric-details__-the-input-and-target-shape-is-not-required-to-have-dense-correspondences-the-same-topology-or-even-the-same-representation-form-eg-points-mesh-and-2d-image&#34;&gt;We can warp an arbitrary input shape to match the grob structure of an arbitrary target shape, while &lt;strong&gt;preserving all the local geometric details&lt;/strong&gt;. The input and target shape is not required to have dense correspondences, the same topology, or even the same representation form (e.g. points, mesh and 2D image).&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;overview.png&#34; alt=&#34;overview&#34;&gt;
The key of our method is reducing the degrees of freedom of the deformation space by extending the traditional cage-based deformation technique.&lt;/p&gt;
&lt;h2 id=&#34;application-1---shape-synthesis&#34;&gt;Application 1 - Shape synthesis&lt;/h2&gt;
&lt;p&gt;We use our method to
learn a meaningful deformation space over a collection of
shapes within the same category, and then use random pairs
of source and target shapes to synthesize plausible variations
of artist-generated assets.
&lt;img src=&#34;comparison.png&#34; alt=&#34;comparison&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;application-2---deformation-transfer&#34;&gt;Application 2 - Deformation transfer&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;deformation_transfer.jpg&#34; alt=&#34;dt&#34;&gt;
We first learn the cage deformation space
for a template source shape (top left) with known pose and body shape
variations. Then, we annotate predefined landmarks on new characters
in neutral poses (left column, rows 2-4). At test time, given novel target
poses (top row, green) without known correspondences to the template, we
transfer their poses to the other characters (blue).&lt;/p&gt;
&lt;p&gt;Due to the agnostic nature of cage-deformations to
the underlying shape, we are able to seamlessly combine
machine learning and traditional geometry processing to
generalize to never-observed characters, even if the novel source and target characters are morphologically very different from the both the template source.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;teaser.jpg&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;talk&#34;&gt;Talk&lt;/h2&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/UZLAj2xTojY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

This is the talk I&amp;rsquo;ll be giving virtually for CVPR2020.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Differentiable Surface Splatting for Point-based Geometry Processing</title>
      <link>https://yifita.github.io/publication/dss/</link>
      <pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://yifita.github.io/publication/dss/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;teaser.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;shape-synthesis&#34;&gt;Shape Synthesis&lt;/h3&gt;
&lt;p&gt;We can synthesize shapes from multiple 2D images. This process is not constrained by topology changes.
&lt;img src=&#34;yoga.png&#34; alt=&#34;synthesis&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;point-cloud-filering&#34;&gt;Point Cloud Filering&lt;/h3&gt;
&lt;p&gt;Using DSS we can directly apply image-based filters to a point cloud to achieve various geometric effect.
&lt;img src=&#34;DSSfilter.png&#34; alt=&#34;filtering&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;point-cloud-denoising&#34;&gt;Point Cloud Denoising&lt;/h3&gt;
&lt;p&gt;We create state-of-the-art point cloud denoising results by marrying our differential renderer with the famous image-to-image translation deep learning framework &lt;em&gt;Pix2Pix&lt;/em&gt;.
&lt;img src=&#34;armadillo_2_all.png&#34; alt=&#34;Denoising&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;accompanying-video&#34;&gt;Accompanying Video&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Q8iTkmIky0o&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h3&gt;
&lt;p&gt;We would like to thank Federico Danieli for the insightful discussion, Philipp Herholz for the timely feedack, Romann Weber for the video voice-over and Derek Liu for the help during the rebuttal.
This work was supported in part by gifts from Adobe, Facebook and Snap, Inc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Shapes</title>
      <link>https://yifita.github.io/project/neural-shape/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://yifita.github.io/project/neural-shape/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Patch-base progressive 3D Point Set Upsampling</title>
      <link>https://yifita.github.io/publication/3pu/</link>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://yifita.github.io/publication/3pu/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;teaser.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;hr&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;hr&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;hr&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;PU-Net&lt;/em&gt;: L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, &amp;ldquo;Pu-net: Point cloud upsampling network&amp;rdquo;, CVPR 2018&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EC-Net&lt;/em&gt;: L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, &amp;ldquo;Ec-net: an edge-aware point set consolidation network&amp;rdquo;,  ECCV 2018&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EAR&lt;/em&gt;: H. Huang, S. Wu, M. Gong, D. Cohen-Or, U. Ascher, and H. Zhang, &amp;ldquo;Edge-aware point set resampling&amp;rdquo;, ACM ToG 2013&lt;/p&gt;
&lt;p&gt;&lt;em&gt;WLOP&lt;/em&gt;: H. Huang, D. Li, H. Zhang, U. Ascher, and D. Cohen-Or, &amp;ldquo;Consolidation of unorganized point clouds for surface reconstruction&amp;rdquo;, SIGGRAPH Asia 2009&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Point-based geometry processing</title>
      <link>https://yifita.github.io/project/point-geometry/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://yifita.github.io/project/point-geometry/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
