<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>1 | Yifan Wang - ETH Zurich</title>
    <link>https://yifita.github.io/publication-type/1/</link>
      <atom:link href="https://yifita.github.io/publication-type/1/index.xml" rel="self" type="application/rss+xml" />
    <description>1</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 10 Mar 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yifita.github.io/media/icon_hu039e7a8dee100027504ac14a50a2ed32_26438_512x512_fill_lanczos_center_3.png</url>
      <title>1</title>
      <link>https://yifita.github.io/publication-type/1/</link>
    </image>
    
    <item>
      <title>Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations</title>
      <link>https://yifita.github.io/publication/iso_points/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://yifita.github.io/publication/iso_points/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/3myC2_BIGcI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;teaser&#34; srcset=&#34;
               /publication/iso_points/teaser_hu80b79292638aae25df499a8d7dc77d01_777796_c4a9801490fc306fbc3acdba65d334fd.png 400w,
               /publication/iso_points/teaser_hu80b79292638aae25df499a8d7dc77d01_777796_572f823d76883eefe2c8da3e349cf268.png 760w,
               /publication/iso_points/teaser_hu80b79292638aae25df499a8d7dc77d01_777796_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://yifita.github.io/publication/iso_points/teaser_hu80b79292638aae25df499a8d7dc77d01_777796_c4a9801490fc306fbc3acdba65d334fd.png&#34;
               width=&#34;739&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

We propose a hybrid neural surface representation with implicit
functions and iso-points. The representation leads to accurate and robust
surface reconstruction from imperfect data. The on-the-fly conversion with
efficient iso-points extraction allows us to augment existing optimization
pipelines in a variety of ways. In the first row, geometry-aware regularizers
are incorporated to reconstruct a surface from a noisy point cloud; in the
second row, geometric details are preserved in multi-view reconstruction
via feature-aware sampling; in the last row, iso-points serve as a 3D prior
to improve the topological accuracy of the reconstructed surface&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;overview&#34; srcset=&#34;
               /publication/iso_points/overview_hu0c4236b94b99942f60443bad6e5b1405_288338_e2eab804711dcd67c898c44440754cfa.png 400w,
               /publication/iso_points/overview_hu0c4236b94b99942f60443bad6e5b1405_288338_30a2abcd8e9beb40b2e712dde4cc2b7a.png 760w,
               /publication/iso_points/overview_hu0c4236b94b99942f60443bad6e5b1405_288338_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://yifita.github.io/publication/iso_points/overview_hu0c4236b94b99942f60443bad6e5b1405_288338_e2eab804711dcd67c898c44440754cfa.png&#34;
               width=&#34;760&#34;
               height=&#34;387&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

We efficiently extract a dense, uniformly distributed set of iso-points as an explicit representation for a neural implicit surface. Since the extraction is fast, iso-points can be integrated back into the optimization as a 3D geometric prior, enhancing the optimization.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Cages for Detail-Preserving 3D Deformations</title>
      <link>https://yifita.github.io/publication/deep_cage/</link>
      <pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://yifita.github.io/publication/deep_cage/</guid>
      <description>&lt;h3 id=&#34;we-can-warp-an-arbitrary-input-shape-to-match-the-grob-structure-of-an-arbitrary-target-shape-while-__preserving-all-the-local-geometric-details__-the-input-and-target-shape-is-not-required-to-have-dense-correspondences-the-same-topology-or-even-the-same-representation-form-eg-points-mesh-and-2d-image&#34;&gt;We can warp an arbitrary input shape to match the grob structure of an arbitrary target shape, while &lt;strong&gt;preserving all the local geometric details&lt;/strong&gt;. The input and target shape is not required to have dense correspondences, the same topology, or even the same representation form (e.g. points, mesh and 2D image).&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;overview&#34; srcset=&#34;
               /publication/deep_cage/overview_huce44a8d917894b13664a2dd0ef2adf56_275854_c7bf1e12bf707e050de7525a93703aab.png 400w,
               /publication/deep_cage/overview_huce44a8d917894b13664a2dd0ef2adf56_275854_36b247ef496add211d704eca55c02d58.png 760w,
               /publication/deep_cage/overview_huce44a8d917894b13664a2dd0ef2adf56_275854_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://yifita.github.io/publication/deep_cage/overview_huce44a8d917894b13664a2dd0ef2adf56_275854_c7bf1e12bf707e050de7525a93703aab.png&#34;
               width=&#34;600&#34;
               height=&#34;277&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

The key of our method is reducing the degrees of freedom of the deformation space by extending the traditional cage-based deformation technique.&lt;/p&gt;
&lt;h2 id=&#34;application-1---shape-synthesis&#34;&gt;Application 1 - Shape synthesis&lt;/h2&gt;
&lt;p&gt;We use our method to
learn a meaningful deformation space over a collection of
shapes within the same category, and then use random pairs
of source and target shapes to synthesize plausible variations
of artist-generated assets.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;comparison&#34; srcset=&#34;
               /publication/deep_cage/comparison_hud82a3f8c98c56d6c297225582a652c0c_618165_593ffdaca5a88ad377145d2a31f23b90.png 400w,
               /publication/deep_cage/comparison_hud82a3f8c98c56d6c297225582a652c0c_618165_4af48eb63123ed4a0cafa467fb9c3fdf.png 760w,
               /publication/deep_cage/comparison_hud82a3f8c98c56d6c297225582a652c0c_618165_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://yifita.github.io/publication/deep_cage/comparison_hud82a3f8c98c56d6c297225582a652c0c_618165_593ffdaca5a88ad377145d2a31f23b90.png&#34;
               width=&#34;760&#34;
               height=&#34;542&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;application-2---deformation-transfer&#34;&gt;Application 2 - Deformation transfer&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;dt&#34; srcset=&#34;
               /publication/deep_cage/deformation_transfer_hucf5be2d167f530b3c6c4ccf6ffd30f6f_1897426_03bcac995bf1957e98ad55eaecfedd0f.jpg 400w,
               /publication/deep_cage/deformation_transfer_hucf5be2d167f530b3c6c4ccf6ffd30f6f_1897426_8d93bff5a67a0157ab6ce07ec3d6b11c.jpg 760w,
               /publication/deep_cage/deformation_transfer_hucf5be2d167f530b3c6c4ccf6ffd30f6f_1897426_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://yifita.github.io/publication/deep_cage/deformation_transfer_hucf5be2d167f530b3c6c4ccf6ffd30f6f_1897426_03bcac995bf1957e98ad55eaecfedd0f.jpg&#34;
               width=&#34;760&#34;
               height=&#34;529&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

We first learn the cage deformation space
for a template source shape (top left) with known pose and body shape
variations. Then, we annotate predefined landmarks on new characters
in neutral poses (left column, rows 2-4). At test time, given novel target
poses (top row, green) without known correspondences to the template, we
transfer their poses to the other characters (blue).&lt;/p&gt;
&lt;p&gt;Due to the agnostic nature of cage-deformations to
the underlying shape, we are able to seamlessly combine
machine learning and traditional geometry processing to
generalize to never-observed characters, even if the novel source and target characters are morphologically very different from the both the template source.&lt;/p&gt;
&lt;h2 id=&#34;talk&#34;&gt;Talk&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/UZLAj2xTojY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Patch-base progressive 3D Point Set Upsampling</title>
      <link>https://yifita.github.io/publication/3pu/</link>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://yifita.github.io/publication/3pu/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;teaser&#34; srcset=&#34;
               /publication/3pu/teaser_hue941d5737ff9a65a92341b6d94789b13_1958401_403fe35eea01c862d9d7782de110b3f2.png 400w,
               /publication/3pu/teaser_hue941d5737ff9a65a92341b6d94789b13_1958401_56fd086f5f1bfed34ccff2acff38b6f4.png 760w,
               /publication/3pu/teaser_hue941d5737ff9a65a92341b6d94789b13_1958401_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://yifita.github.io/publication/3pu/teaser_hue941d5737ff9a65a92341b6d94789b13_1958401_403fe35eea01c862d9d7782de110b3f2.png&#34;
               width=&#34;760&#34;
               height=&#34;148&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;center&gt;
$16\times$ upsampling from 625 points
&lt;/center&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/3pu/figure_sparse_hud2ea52a596ae36c36eb01f40e00c846e_1836455_46ecceca49bf37b93f376b6d786f94bd.png 400w,
               /publication/3pu/figure_sparse_hud2ea52a596ae36c36eb01f40e00c846e_1836455_3c2ed8e0adfe10d3d734d9f807780863.png 760w,
               /publication/3pu/figure_sparse_hud2ea52a596ae36c36eb01f40e00c846e_1836455_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://yifita.github.io/publication/3pu/figure_sparse_hud2ea52a596ae36c36eb01f40e00c846e_1836455_46ecceca49bf37b93f376b6d786f94bd.png&#34;
               width=&#34;760&#34;
               height=&#34;343&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;center&gt;$16\times$ upsampling from 5000 points&lt;/center&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/3pu/figure_dense_hu014ad1967466bfe0de5a18b8abf9e18d_3850114_40e045a89003dcd4995f9dd5d4b5d65e.png 400w,
               /publication/3pu/figure_dense_hu014ad1967466bfe0de5a18b8abf9e18d_3850114_07a54e6eed0cb5f41d1c5da462547054.png 760w,
               /publication/3pu/figure_dense_hu014ad1967466bfe0de5a18b8abf9e18d_3850114_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://yifita.github.io/publication/3pu/figure_dense_hu014ad1967466bfe0de5a18b8abf9e18d_3850114_40e045a89003dcd4995f9dd5d4b5d65e.png&#34;
               width=&#34;760&#34;
               height=&#34;531&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;center&gt;$16\times$ upsampling from scan data&lt;/center&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/3pu/real_scan_hu291df38f9bd4033062d7137c94f34fb5_4162448_777fe85ae13531d32d6456a4a9f4f382.png 400w,
               /publication/3pu/real_scan_hu291df38f9bd4033062d7137c94f34fb5_4162448_4122e220a57ca94df431a51ac1f9a94b.png 760w,
               /publication/3pu/real_scan_hu291df38f9bd4033062d7137c94f34fb5_4162448_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://yifita.github.io/publication/3pu/real_scan_hu291df38f9bd4033062d7137c94f34fb5_4162448_777fe85ae13531d32d6456a4a9f4f382.png&#34;
               width=&#34;760&#34;
               height=&#34;687&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;center&gt;$16\times$ upsampling from virtual scan data&lt;/center&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/3pu/scan_hud8ec081405baf5878204f29fd86bddd9_1766035_157938174fe19d0f82fe7f5812222c09.png 400w,
               /publication/3pu/scan_hud8ec081405baf5878204f29fd86bddd9_1766035_fb3ba9a67f7c936b616280273a5bcbd4.png 760w,
               /publication/3pu/scan_hud8ec081405baf5878204f29fd86bddd9_1766035_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://yifita.github.io/publication/3pu/scan_hud8ec081405baf5878204f29fd86bddd9_1766035_157938174fe19d0f82fe7f5812222c09.png&#34;
               width=&#34;704&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;PU-Net&lt;/em&gt;: L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, &amp;ldquo;Pu-net: Point cloud upsampling network&amp;rdquo;, CVPR 2018&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EC-Net&lt;/em&gt;: L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, &amp;ldquo;Ec-net: an edge-aware point set consolidation network&amp;rdquo;,  ECCV 2018&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EAR&lt;/em&gt;: H. Huang, S. Wu, M. Gong, D. Cohen-Or, U. Ascher, and H. Zhang, &amp;ldquo;Edge-aware point set resampling&amp;rdquo;, ACM ToG 2013&lt;/p&gt;
&lt;p&gt;&lt;em&gt;WLOP&lt;/em&gt;: H. Huang, D. Li, H. Zhang, U. Ascher, and D. Cohen-Or, &amp;ldquo;Consolidation of unorganized point clouds for surface reconstruction&amp;rdquo;, SIGGRAPH Asia 2009&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Fully Progressive Approach to Single-Image Super-Resolution</title>
      <link>https://yifita.github.io/publication/prosr/</link>
      <pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://yifita.github.io/publication/prosr/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;teaser&#34; srcset=&#34;
               /publication/prosr/teaser_hu7bc39c5843791f2e96e2bf4a6d0ee2c4_908466_2e722302f31b6190be461d5b785c3372.png 400w,
               /publication/prosr/teaser_hu7bc39c5843791f2e96e2bf4a6d0ee2c4_908466_d1d9e47169d4c470fc9eb628112c6528.png 760w,
               /publication/prosr/teaser_hu7bc39c5843791f2e96e2bf4a6d0ee2c4_908466_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://yifita.github.io/publication/prosr/teaser_hu7bc39c5843791f2e96e2bf4a6d0ee2c4_908466_2e722302f31b6190be461d5b785c3372.png&#34;
               width=&#34;760&#34;
               height=&#34;213&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;benchmark&#34; srcset=&#34;
               /publication/prosr/prosr_benchmark_hu0c14cc2d175ff2dbd1dc10cd687365c1_129303_8cfd438c60ac5ceb4548dd2fc869808f.png 400w,
               /publication/prosr/prosr_benchmark_hu0c14cc2d175ff2dbd1dc10cd687365c1_129303_2bfc699c87d1cdb335bfccb105be4071.png 760w,
               /publication/prosr/prosr_benchmark_hu0c14cc2d175ff2dbd1dc10cd687365c1_129303_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://yifita.github.io/publication/prosr/prosr_benchmark_hu0c14cc2d175ff2dbd1dc10cd687365c1_129303_8cfd438c60ac5ceb4548dd2fc869808f.png&#34;
               width=&#34;760&#34;
               height=&#34;154&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;MsLapSRN: Lai, Wei-Sheng, et al. &amp;ldquo;Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks.&amp;rdquo; arXiv preprint arXiv:1710.01992 (2017).&lt;/p&gt;
&lt;p&gt;EDSR, MDSR: Lim, Bee, et al. &amp;ldquo;Enhanced deep residual networks for single image super-resolution.&amp;rdquo; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. Vol. 1. No. 2. 2017.&lt;/p&gt;
&lt;p&gt;RDN:Zhang, Yulun, et al. &amp;ldquo;Residual Dense Network for Image Super-Resolution.&amp;rdquo;  The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- ### Accompanying Video ###
(Best viewed in full-screen mode)

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/ON3XHnaDepE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
&lt;h3 id=&#34;featured-video&#34;&gt;Featured Video&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s a pleasure to be featured in &amp;ldquo;&lt;a href=&#34;https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2 minute paper&lt;/a&gt;&amp;rdquo;, an amazing YouTube channel that introduces latest development of AI in a variety of applications.
Here&amp;rsquo;s the video that talks about our work.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/HvH0b9K_Iro&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two-Stream SR-CNNs for Action Recognition in Videos</title>
      <link>https://yifita.github.io/publication/action_srcnn/</link>
      <pubDate>Fri, 01 Jul 2016 00:00:00 +0000</pubDate>
      <guid>https://yifita.github.io/publication/action_srcnn/</guid>
      <description>&lt;h3 id=&#34;network-architecture&#34;&gt;Network Architecture&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Architecture&#34; srcset=&#34;
               /publication/action_srcnn/featured_hu1e735c8506cd55df4c6fe72ecd504b14_61342_b1318c6396a224220e8c881567054a19.jpg 400w,
               /publication/action_srcnn/featured_hu1e735c8506cd55df4c6fe72ecd504b14_61342_3fb4c7052b6d511caca20476b3a561c3.jpg 760w,
               /publication/action_srcnn/featured_hu1e735c8506cd55df4c6fe72ecd504b14_61342_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://yifita.github.io/publication/action_srcnn/featured_hu1e735c8506cd55df4c6fe72ecd504b14_61342_b1318c6396a224220e8c881567054a19.jpg&#34;
               width=&#34;760&#34;
               height=&#34;337&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
